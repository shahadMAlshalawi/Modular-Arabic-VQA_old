{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk6ceO1XERd3",
        "outputId": "c730aea4-aa87-47b4-a473-8326f2f4d8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrjCxND2IamL"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sse1uQOmXF8z"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dataset_size, num_segments = 214354, 214\n",
        "segment_size, remainder = divmod(dataset_size, num_segments)\n",
        "size_segments = [\n",
        "    range(i * segment_size + min(i, remainder), (i + 1) * segment_size + min(i + 1, remainder))\n",
        "    for i in range(num_segments)\n",
        "]\n",
        "size_segments\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFnpb6aZxgzF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class GeminiConfig:\n",
        "  \"\"\"\n",
        "  Configuration for Gemini.\n",
        "  Contains default parameters that can be used globally.\n",
        "  \"\"\"\n",
        "\n",
        "  # General settings\n",
        "\n",
        "\n",
        "  # Model settings\n",
        "  MODEL_NAME = \"models/gemini-1.5-flash\"  #  Model name\n",
        "  # Generation settings\n",
        "  MAX_LENGTH = 20  # Maximum length of generated sequences\n",
        "\n",
        "  # Dataset settings\n",
        "  DATASET_PATH = \"ShahadMAlshalawi/vqav2-ar-validation-data\"  # Path or name of the dataset\n",
        "  LANGUAGE = \"ar\"  # Language for questions/answers (\"ar\" for Arabic, \"en\" for English)\n",
        "  SPLIT = \"validation\"  # Dataset split to use (\"train\", \"validation\", \"test\")\n",
        "  SAVE_SEGMENT_DIR = \"/content/drive/MyDrive/ColabData/Gemini_VQA_Results/VQAv2-ar/VQA_generation_checkpoint\"  # Directory to save extracted answers\n",
        "\n",
        "  SIZE_SEGMENTS = [\n",
        "      range(0, 1002),\n",
        "      range(1002, 2004),\n",
        "      range(2004, 3006),\n",
        "      range(3006, 4008),\n",
        "      range(4008, 5010),\n",
        "      range(5010, 6012),\n",
        "      range(6012, 7014),\n",
        "      range(7014, 8016),\n",
        "      range(8016, 9018),\n",
        "      range(9018, 10020),\n",
        "      range(10020, 11022),\n",
        "      range(11022, 12024),\n",
        "      range(12024, 13026),\n",
        "      range(13026, 14028),\n",
        "      range(14028, 15030),\n",
        "      range(15030, 16032),\n",
        "      range(16032, 17034),\n",
        "      range(17034, 18036),\n",
        "      range(18036, 19038),\n",
        "      range(19038, 20040),\n",
        "      range(20040, 21042),\n",
        "      range(21042, 22044),\n",
        "      range(22044, 23046),\n",
        "      range(23046, 24048),\n",
        "      range(24048, 25050),\n",
        "      range(25050, 26052),\n",
        "      range(26052, 27054),\n",
        "      range(27054, 28056),\n",
        "      range(28056, 29058),\n",
        "      range(29058, 30060),\n",
        "      range(30060, 31062),\n",
        "      range(31062, 32064),\n",
        "      range(32064, 33066),\n",
        "      range(33066, 34068),\n",
        "      range(34068, 35070),\n",
        "      range(35070, 36072),\n",
        "      range(36072, 37074),\n",
        "      range(37074, 38076),\n",
        "      range(38076, 39078),\n",
        "      range(39078, 40080),\n",
        "      range(40080, 41082),\n",
        "      range(41082, 42084),\n",
        "      range(42084, 43086),\n",
        "      range(43086, 44088),\n",
        "      range(44088, 45090),\n",
        "      range(45090, 46092),\n",
        "      range(46092, 47094),\n",
        "      range(47094, 48096),\n",
        "      range(48096, 49098),\n",
        "      range(49098, 50100),\n",
        "      range(50100, 51102),\n",
        "      range(51102, 52104),\n",
        "      range(52104, 53106),\n",
        "      range(53106, 54108),\n",
        "      range(54108, 55110),\n",
        "      range(55110, 56112),\n",
        "      range(56112, 57114),\n",
        "      range(57114, 58116),\n",
        "      range(58116, 59118),\n",
        "      range(59118, 60120),\n",
        "      range(60120, 61122),\n",
        "      range(61122, 62124),\n",
        "      range(62124, 63126),\n",
        "      range(63126, 64128),\n",
        "      range(64128, 65130),\n",
        "      range(65130, 66132),\n",
        "      range(66132, 67134),\n",
        "      range(67134, 68136),\n",
        "      range(68136, 69138),\n",
        "      range(69138, 70140),\n",
        "      range(70140, 71142),\n",
        "      range(71142, 72144),\n",
        "      range(72144, 73146),\n",
        "      range(73146, 74148),\n",
        "      range(74148, 75150),\n",
        "      range(75150, 76152),\n",
        "      range(76152, 77154),\n",
        "      range(77154, 78156),\n",
        "      range(78156, 79158),\n",
        "      range(79158, 80160),\n",
        "      range(80160, 81162),\n",
        "      range(81162, 82164),\n",
        "      range(82164, 83166),\n",
        "      range(83166, 84168),\n",
        "      range(84168, 85170),\n",
        "      range(85170, 86172),\n",
        "      range(86172, 87174),\n",
        "      range(87174, 88176),\n",
        "      range(88176, 89178),\n",
        "      range(89178, 90180),\n",
        "      range(90180, 91182),\n",
        "      range(91182, 92184),\n",
        "      range(92184, 93186),\n",
        "      range(93186, 94188),\n",
        "      range(94188, 95190),\n",
        "      range(95190, 96192),\n",
        "      range(96192, 97194),\n",
        "      range(97194, 98196),\n",
        "      range(98196, 99198),\n",
        "      range(99198, 100200),\n",
        "      range(100200, 101202),\n",
        "      range(101202, 102204),\n",
        "      range(102204, 103206),\n",
        "      range(103206, 104208),\n",
        "      range(104208, 105210),\n",
        "      range(105210, 106212),\n",
        "      range(106212, 107214),\n",
        "      range(107214, 108216),\n",
        "      range(108216, 109218),\n",
        "      range(109218, 110220),\n",
        "      range(110220, 111222),\n",
        "      range(111222, 112224),\n",
        "      range(112224, 113226),\n",
        "      range(113226, 114228),\n",
        "      range(114228, 115230),\n",
        "      range(115230, 116232),\n",
        "      range(116232, 117234),\n",
        "      range(117234, 118236),\n",
        "      range(118236, 119238),\n",
        "      range(119238, 120240),\n",
        "      range(120240, 121242),\n",
        "      range(121242, 122244),\n",
        "      range(122244, 123246),\n",
        "      range(123246, 124248),\n",
        "      range(124248, 125250),\n",
        "      range(125250, 126252),\n",
        "      range(126252, 127254),\n",
        "      range(127254, 128256),\n",
        "      range(128256, 129258),\n",
        "      range(129258, 130260),\n",
        "      range(130260, 131262),\n",
        "      range(131262, 132264),\n",
        "      range(132264, 133266),\n",
        "      range(133266, 134268),\n",
        "      range(134268, 135270),\n",
        "      range(135270, 136272),\n",
        "      range(136272, 137274),\n",
        "      range(137274, 138276),\n",
        "      range(138276, 139278),\n",
        "      range(139278, 140280),\n",
        "      range(140280, 141281),\n",
        "      range(141281, 142282),\n",
        "      range(142282, 143283),\n",
        "      range(143283, 144284),\n",
        "      range(144284, 145285),\n",
        "      range(145285, 146286),\n",
        "      range(146286, 147287),\n",
        "      range(147287, 148288),\n",
        "      range(148288, 149289),\n",
        "      range(149289, 150290),\n",
        "      range(150290, 151291),\n",
        "      range(151291, 152292),\n",
        "      range(152292, 153293),\n",
        "      range(153293, 154294),\n",
        "      range(154294, 155295),\n",
        "      range(155295, 156296),\n",
        "      range(156296, 157297),\n",
        "      range(157297, 158298),\n",
        "      range(158298, 159299),\n",
        "      range(159299, 160300),\n",
        "      range(160300, 161301),\n",
        "      range(161301, 162302),\n",
        "      range(162302, 163303),\n",
        "      range(163303, 164304),\n",
        "      range(164304, 165305),\n",
        "      range(165305, 166306),\n",
        "      range(166306, 167307),\n",
        "      range(167307, 168308),\n",
        "      range(168308, 169309),\n",
        "      range(169309, 170310),\n",
        "      range(170310, 171311),\n",
        "      range(171311, 172312),\n",
        "      range(172312, 173313),\n",
        "      range(173313, 174314),\n",
        "      range(174314, 175315),\n",
        "      range(175315, 176316),\n",
        "      range(176316, 177317),\n",
        "      range(177317, 178318),\n",
        "      range(178318, 179319),\n",
        "      range(179319, 180320),\n",
        "      range(180320, 181321),\n",
        "      range(181321, 182322),\n",
        "      range(182322, 183323),\n",
        "      range(183323, 184324),\n",
        "      range(184324, 185325),\n",
        "      range(185325, 186326),\n",
        "      range(186326, 187327),\n",
        "      range(187327, 188328),\n",
        "      range(188328, 189329),\n",
        "      range(189329, 190330),\n",
        "      range(190330, 191331),\n",
        "      range(191331, 192332),\n",
        "      range(192332, 193333),\n",
        "      range(193333, 194334),\n",
        "      range(194334, 195335),\n",
        "      range(195335, 196336),\n",
        "      range(196336, 197337),\n",
        "      range(197337, 198338),\n",
        "      range(198338, 199339),\n",
        "      range(199339, 200340),\n",
        "      range(200340, 201341),\n",
        "      range(201341, 202342),\n",
        "      range(202342, 203343),\n",
        "      range(203343, 204344),\n",
        "      range(204344, 205345),\n",
        "      range(205345, 206346),\n",
        "      range(206346, 207347),\n",
        "      range(207347, 208348),\n",
        "      range(208348, 209349),\n",
        "      range(209349, 210350),\n",
        "      range(210350, 211351),\n",
        "      range(211351, 212352),\n",
        "      range(212352, 213353),\n",
        "      range(213353, 214354)\n",
        "      ]\n",
        "\n",
        "\n",
        "  CURRENT_INDEX_SEGMENT = 0  # Index of the current segment being processed\n",
        "  USERNAME = \"ShahadMAlshalawi\"  # Username for Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE74fPd0TIWr"
      },
      "source": [
        "# Gemini VQA Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl7biewsTM1N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Union, Iterable, Dict\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "class GeminiVQA:\n",
        "    \"\"\"\n",
        "    Implementation of Visual Question Answering using Gemini via the Google AI Gemini API.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str, max_tokens: int = 1024):\n",
        "        \"\"\"\n",
        "        Initializes the Gemini VQA model.\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Google AI Gemini API key.\n",
        "            model_name (str): The name of the Gemini model to use.\n",
        "            max_tokens (int): The maximum number of tokens to generate in the answer.\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        genai.configure(api_key=self.api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.model_name = model_name\n",
        "        self.max_tokens = max_tokens\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _prepare_image(self, image: Union[str, np.ndarray, torch.Tensor, Image.Image]) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Prepares a single image for VQA, converting it to RGB format. Handles various input types.\n",
        "\n",
        "        Args:\n",
        "            image: The input image. Can be a file path, URL, NumPy array, PyTorch tensor, or PIL Image.\n",
        "\n",
        "        Returns:\n",
        "            A PIL Image object in RGB format.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the input image type is unsupported or if an error occurs during image processing.\n",
        "            requests.exceptions.RequestException: If there's an error downloading the image from a URL.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            if isinstance(image, Image.Image):\n",
        "                return image.convert(\"RGB\")\n",
        "\n",
        "            elif isinstance(image, str):\n",
        "                if image.startswith(\"http\"):\n",
        "                    response = requests.get(image, stream=True)\n",
        "                    response.raise_for_status()\n",
        "                    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                else:\n",
        "                    return Image.open(image).convert(\"RGB\")\n",
        "\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                return Image.fromarray(image).convert(\"RGB\")\n",
        "\n",
        "            elif torch.is_tensor(image):\n",
        "                return Image.fromarray(image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)).convert(\"RGB\")\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported image input type: {type(image)}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise requests.exceptions.RequestException(f\"Error downloading image: {e}\")\n",
        "\n",
        "    def prepare_inputs(self, image: Union[str, np.ndarray, Image.Image], question: str):\n",
        "        \"\"\"\n",
        "        Prepares the image and question for the Gemini model, requesting the answer in Arabic.\n",
        "\n",
        "        Args:\n",
        "            image: The input image.\n",
        "            question (str): The question about the image.\n",
        "\n",
        "        Returns:\n",
        "            List: A list containing the prepared image and the question with an Arabic instruction.\n",
        "        \"\"\"\n",
        "        prepared_image = self._prepare_image(image)\n",
        "        prompt = f\"You are an AI assistant that generates accurate answer about image in Arabic.\\nQuestion: {question}\"\n",
        "        return [prepared_image, prompt]\n",
        "\n",
        "\n",
        "    def answer_question(self, image: Union[str, np.ndarray, Image.Image], question: str) -> str:\n",
        "        \"\"\"\n",
        "        Answers a question about an image using the Gemini API.\n",
        "\n",
        "        Args:\n",
        "            image: The input image.\n",
        "            question (str): The question about the image.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated answer.\n",
        "        \"\"\"\n",
        "        prompt_parts = self.prepare_inputs(image, question)\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt_parts, request_options={'timeout': 600}, generation_config={\n",
        "                \"max_output_tokens\": self.max_tokens\n",
        "            })\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {e}\")\n",
        "            return \"Error generating answer\"\n",
        "\n",
        "    def __call__(self, image: Union[str, np.ndarray, Image.Image], question: str) -> str:\n",
        "        \"\"\"\n",
        "        Answers a question about an image using the Gemini API.\n",
        "\n",
        "        Args:\n",
        "            image: The input image.\n",
        "            question (str): The question about the image.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated answer.\n",
        "        \"\"\"\n",
        "        return self.answer_question(image, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kNbNFEK5mU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_hrZM3kKiSw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc8ZN0wImZh0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggmYy_0RLgCY"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcM1d2qMmHau"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(GeminiConfig.DATASET_PATH,split=GeminiConfig.SPLIT,trust_remote_code=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fE1l-lRmiJc",
        "outputId": "f99f0ef3-f5b7-4023-b424-f77af858496d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'image_id': 262148, 'question_id': 262148000, 'question_type': 'none of the above', 'answer_type': 'other'}, 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x512 at 0x7E2D3815A3F0>, 'question': {'en': 'Where is he looking?', 'ar': 'أين ينظر؟'}, 'answers': {'en': ['down', 'down', 'at table', 'skateboard', 'down', 'table', 'down', 'down', 'down', 'down'], 'ar': ['تحت', 'تحت', 'على الطاولة', 'لوح التزلج', 'تحت', 'طاولة', 'تحت', 'تحت', 'تحت', 'تحت'], 'confidence': ['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes'], 'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, 'multiple_choice_answer': {'en': 'down', 'ar': 'تحت'}}\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RFXIs7eXPms"
      },
      "source": [
        "# Genearte Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qQ3ntM0hXV6e",
        "outputId": "11b3af1f-8143-4de4-aab7-316dcd6c552e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: ما نوع السيارة التي تستعمل الشيء الظاهر في الصورة؟\n",
            "Answer: أنا آسف، ولكن الصورة التي قدمتها لا تحتوي على أي سيارة. الصورة تظهر صنبور إطفاء.\n"
          ]
        }
      ],
      "source": [
        "api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "if api_key is None:\n",
        "    print(\"Error: GEMINI_API_KEY not found. Please set the GEMINI_API_KEY environment variable or pass the api key as a parameter to the constructor.\")\n",
        "else:\n",
        "    vqa_model = GeminiVQA(api_key=api_key, model_name=GeminiConfig.MODEL_NAME)\n",
        "\n",
        "    # Example usage with an image and question\n",
        "    image_url = \"/content/drive/MyDrive/Colab Notebooks/E1.png\"\n",
        "    question = \"ما نوع السيارة التي تستعمل الشيء الظاهر في الصورة؟\"\n",
        "\n",
        "    try:\n",
        "        answer = vqa_model(image_url, question)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BJ0dLajdkb8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Process set of segments using loop\n",
        "for current_index_segment in range(176, 180):  # Loop through segments 0 to 30\n",
        "    print(f\"Processing segment {current_index_segment}...\")\n",
        "    rng = GeminiConfig.SIZE_SEGMENTS[current_index_segment]\n",
        "    data_segment = dataset.select(rng)\n",
        "\n",
        "    outputs = {\n",
        "        \"question_id\": [],\n",
        "        \"questions\": [],\n",
        "        \"image_id\": [],\n",
        "        \"answers\": [],\n",
        "        \"predictions\": []\n",
        "    }\n",
        "\n",
        "    for item in tqdm(data_segment, desc=f\"Generating answers for segment {current_index_segment}\"):\n",
        "        try:\n",
        "            question = item['question']['ar'] if GeminiConfig.LANGUAGE == 'ar' else item['question']['en']\n",
        "            ground_truth_answers = item['answers']['ar'] if GeminiConfig.LANGUAGE == 'ar' else item['answers']['en']\n",
        "            image = item['image']\n",
        "            question_id = str(item['metadata']['question_id'])\n",
        "            image_id = str(item['metadata']['image_id'])\n",
        "\n",
        "            prediction = vqa_model(image, question)\n",
        "\n",
        "            outputs[\"question_id\"].append(question_id)\n",
        "            outputs[\"questions\"].append(question) # Append the question here\n",
        "            outputs[\"image_id\"].append(image_id)\n",
        "            outputs[\"answers\"].append(ground_truth_answers)\n",
        "            outputs[\"predictions\"].append(prediction)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {item.get('metadata', {}).get('question_id', 'N/A')}: {e}\")\n",
        "            outputs[\"question_id\"].append(str(item.get('metadata', {}).get('question_id', 'N/A')))\n",
        "            outputs[\"questions\"].append(\"N/A\") # Append N/A for question if error\n",
        "            outputs[\"image_id\"].append(str(item.get('metadata', {}).get('image_id', 'N/A')))\n",
        "            outputs[\"answers\"].append([])  # Append empty list for ground truth if error\n",
        "            outputs[\"predictions\"].append(\"Error generating answer\") # Indicate error in prediction\n",
        "\n",
        "\n",
        "    #saving the predictions to a JSON file\n",
        "    save_path = f\"{GeminiConfig.SAVE_SEGMENT_DIR}/segment_{current_index_segment}_outputs.json\"\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(outputs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Outputs saved to {save_path}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAtE6lofViXd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-RLxayoAJtp"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFrSQ8GTAJtq"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evNU1EL4AJtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5f560b-966c-4632-a50d-7e581652c518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for aravqa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for vinvl_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Violet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for vinvl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for args (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/shahadMAlshalawi/Modular-Arabic-VQA.git --no-warn-conflicts --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee-riJnbAJtr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import textwrap\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import evaluate\n",
        "import json\n",
        "import datasets\n",
        "import aravqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gkLCFM3WIn0"
      },
      "source": [
        "### Loading the data from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc9fdciF_-qA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Directory containing the saved JSON output files\n",
        "output_dir = GeminiConfig.SAVE_SEGMENT_DIR\n",
        "\n",
        "# Initialize a dictionary to store the combined outputs with the correct keys\n",
        "outputs = {\n",
        "    \"question_id\": [],\n",
        "    \"questions\": [],\n",
        "    \"image_id\": [],\n",
        "    \"answers\": [],\n",
        "    \"predictions\": []\n",
        "}\n",
        "\n",
        "# Iterate through the files in the directory and load the JSON data\n",
        "for filename in sorted(os.listdir(output_dir)):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Append the data from each file to the combined outputs dictionary\n",
        "            for key in outputs.keys():\n",
        "                if key in data:  # Check if the key exists in the loaded data\n",
        "                    outputs[key].extend(data[key])\n",
        "                else:\n",
        "                    # This case should not happen with the corrected keys, but good practice to handle\n",
        "                    print(f\"Warning: Key '{key}' not found in file '{filename}'. Skipping.\")\n",
        "\n",
        "\n",
        "# Display the first few entries of the combined outputs for verification\n",
        "print(\"Combined outputs loaded successfully. First 5 entries:\")\n",
        "print(outputs['predictions'][:5])\n",
        "print(outputs['questions'][:5])\n",
        "print(outputs['answers'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7-f7_s2qr0t"
      },
      "outputs": [],
      "source": [
        "# Print the last 5 entries of the combined outputs for verification\n",
        "print(\"Combined outputs loaded successfully. Last 5 entries:\")\n",
        "print(outputs['predictions'][-2:])\n",
        "print(outputs['questions'][-2:])\n",
        "print(outputs['answers'][-2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWjb8f5o_U8z"
      },
      "source": [
        "## Evaluate in segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9NKb7nnAJts"
      },
      "outputs": [],
      "source": [
        "from aravqa.modules.evaluation import BLEUEvaluator\n",
        "from aravqa.modules.evaluation import BERTScoreEvaluator\n",
        "from aravqa.modules.evaluation import FuzzEvaluator\n",
        "import os\n",
        "\n",
        "# Define the directory to save segmented results\n",
        "segmented_results_dir = f\"/content/drive/MyDrive/ColabData/Gemini_VQA_Results/VQAv2-ar/Evaluation_Results/\"\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('GPT_API_KEY')\n",
        "\n",
        "# Initialize evaluators\n",
        "bleu_evaluator = BLEUEvaluator(max_order=1)\n",
        "BertScore_evaluator = BERTScoreEvaluator()\n",
        "fuzzy_evaluator = FuzzEvaluator(OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "# Iterate through segments starting from segment 0\n",
        "for i, rng in enumerate(GeminiConfig.SIZE_SEGMENTS[0:], start=0):\n",
        "    print(f\"Evaluating segment {i}...\")\n",
        "\n",
        "    # Select the data for the current segment\n",
        "    segment_outputs = {\n",
        "        \"question_id\": outputs[\"question_id\"][rng.start:rng.stop],\n",
        "        \"questions\": outputs[\"questions\"][rng.start:rng.stop],\n",
        "        \"image_id\": outputs[\"image_id\"][rng.start:rng.stop],\n",
        "        \"answers\": outputs[\"answers\"][rng.start:rng.stop],\n",
        "        \"predictions\": outputs[\"predictions\"][rng.start:rng.stop]\n",
        "    }\n",
        "\n",
        "    # Perform evaluations for the current segment\n",
        "    bleu_results = bleu_evaluator.evaluate(predictions=segment_outputs['predictions'],\n",
        "                                           references=segment_outputs['answers']\n",
        "                                           )\n",
        "\n",
        "    bertScore_results = BertScore_evaluator.evaluate(predictions=segment_outputs['predictions'],\n",
        "                                                   references=segment_outputs['answers']\n",
        "                                                   )\n",
        "\n",
        "    fuzzy_results = fuzzy_evaluator.evaluate(predictions=segment_outputs['predictions'],\n",
        "                                           references=segment_outputs['answers'],\n",
        "                                             questions=segment_outputs['questions']\n",
        "                                           )\n",
        "\n",
        "    # Prepare segment results dictionary\n",
        "    segment_eval_results = {\n",
        "        'question_id': segment_outputs['question_id'],\n",
        "        'questions': segment_outputs['questions'],\n",
        "        'image_id': segment_outputs['image_id'],\n",
        "        'answers': segment_outputs['answers'],\n",
        "        'predictions': segment_outputs['predictions'],\n",
        "        'bleu': bleu_results['bleu'],\n",
        "        'f1_bertscore': bertScore_results['f1_bertscore'],\n",
        "        'fuzz_accuracy': fuzzy_results['fuzz_accuracy'],\n",
        "    }\n",
        "\n",
        "    # Add overall metrics to the first row of the segment results\n",
        "    if len(segment_outputs['question_id']) > 0:\n",
        "        segment_eval_results['overall_bleu'] = [bleu_results['overall_bleu']] + [None] * (len(segment_outputs['question_id']) - 1)\n",
        "        segment_eval_results['overall_f1_bertscore'] = [bertScore_results['overall_f1_bertscore']] + [None] * (len(segment_outputs['question_id']) - 1)\n",
        "        segment_eval_results['fuzz_overall_accuracy'] = [fuzzy_results['fuzz_overall_accuracy']] + [None] * (len(segment_outputs['question_id']) - 1)\n",
        "    else:\n",
        "        segment_eval_results['overall_bleu'] = []\n",
        "        segment_eval_results['overall_f1_bertscore'] = []\n",
        "        segment_eval_results['fuzz_overall_accuracy'] = []\n",
        "\n",
        "\n",
        "    # Create a DataFrame and save the segment results to a CSV file\n",
        "    segment_df = pd.DataFrame.from_dict(segment_eval_results)\n",
        "    segment_filename = f\"segment_{i}_results.csv\"\n",
        "    segment_file_path = os.path.join(segmented_results_dir, segment_filename)\n",
        "    segment_df.to_csv(segment_file_path, index=False)\n",
        "\n",
        "    print(f\"Segment {i} results saved to {segment_file_path}\")\n",
        "\n",
        "print(\"\\nEvaluation of all segments completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}